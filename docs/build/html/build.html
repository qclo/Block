<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>BLOCK Installation &mdash; BLOCK 1.5.0 documentation</title>
    
    <link rel="stylesheet" href="static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.5.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="static/jquery.js"></script>
    <script type="text/javascript" src="static/underscore.js"></script>
    <script type="text/javascript" src="static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="BLOCK 1.5.0 documentation" href="index.html" />
    <link rel="next" title="Typical Calculations with BLOCK" href="examples.html" />
    <link rel="prev" title="Overview" href="overview.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="examples.html" title="Typical Calculations with BLOCK"
             accesskey="N">next</a></li>
        <li class="right" >
          <a href="overview.html" title="Overview"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">BLOCK 1.5.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#"><cite>BLOCK</cite> Installation</a><ul>
<li><a class="reference internal" href="#compile">Compile</a><ul>
<li><a class="reference internal" href="#interface-to-pyscf-package">Interface to PySCF package</a></li>
</ul>
</li>
<li><a class="reference internal" href="#how-to-run-block">How to run <cite>BLOCK</cite></a></li>
<li><a class="reference internal" href="#testjobs">Testjobs</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="overview.html"
                        title="previous chapter">Overview</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="examples.html"
                        title="next chapter">Typical Calculations with <cite>BLOCK</cite></a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="sources/build.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="block-installation">
<h1><cite>BLOCK</cite> Installation<a class="headerlink" href="#block-installation" title="Permalink to this headline">¶</a></h1>
<div class="section" id="compile">
<h2>Compile<a class="headerlink" href="#compile" title="Permalink to this headline">¶</a></h2>
<p><cite>BLOCK</cite> requires BLAS, LAPACK and BOOST.
MPI library is needed for distributed-memory parallel compilation.
<cite>BLOCK</cite> is compiled using the makefile supplied in the distribution.
The following customizations need to be made to the makefile placed in the main directory <tt class="docutils literal"><span class="pre">./Block</span></tt>.</p>
<dl class="docutils">
<dt>Choose compilers by specifying</dt>
<dd><tt class="docutils literal"><span class="pre">CXX</span> <span class="pre">=</span> <span class="pre">g++</span></tt></dd>
<dt>For MPI-based parallel execution on distributed-memory machines,</dt>
<dd><p class="first"><tt class="docutils literal"><span class="pre">USE_MPI</span> <span class="pre">=</span> <span class="pre">yes</span></tt></p>
<p class="last"><tt class="docutils literal"><span class="pre">MPICXX</span> <span class="pre">=</span> <span class="pre">mpicxx</span></tt></p>
</dd>
</dl>
<p>MPI library must be compiled using the same compiler as for compiling <cite>BLOCK</cite>.
Intel compiler such as <tt class="docutils literal"><span class="pre">icpc</span></tt> is also supported with approriate compiling flags chosen automatically.</p>
<p>Please note that when choosing your compiler, either GNU or Intel, C++0x/C++11 standards must be appropriately supported,
as <cite>BLOCK</cite> requires new features for some of the modules (eg, <cite>npdm</cite>, <cite>nevpt2</cite>, etc).
Here are our suggested <cite>minimum</cite> GNU/Intel compiler versions in order for the compiling process to be successful:</p>
<ul class="simple">
<li>GNU <tt class="docutils literal"><span class="pre">g++</span></tt>: 4.8 or newer,</li>
<li>or Intel <tt class="docutils literal"><span class="pre">icpc</span></tt>: at least 14.0.1 (2013 SP1 Update 1) or newer.</li>
</ul>
<p>To enable MKL library,</p>
<blockquote>
<div><tt class="docutils literal"><span class="pre">USE_MKL</span> <span class="pre">=</span> <span class="pre">yes</span></tt></div></blockquote>
<p>And supply MKL and BOOST libraries by giving the locations,</p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">MKLLIB</span> <span class="pre">=</span> <span class="pre">/opt/intel/composer_xe_2013_sp1.0.080/mkl/lib/intel64/</span></tt></p>
<p><tt class="docutils literal"><span class="pre">MKLFLAGS</span> <span class="pre">=</span> <span class="pre">/opt/intel/composer_xe_2013_sp1.0.080/mkl/include</span></tt></p>
<p><tt class="docutils literal"><span class="pre">BOOSTLIB</span> <span class="pre">=</span> <span class="pre">/lib64/boost_1_55_0/lib/</span></tt></p>
<p><tt class="docutils literal"><span class="pre">BOOSTINCLUDE</span> <span class="pre">=</span> <span class="pre">/lib64/boost_1_55_0/include/</span></tt></p>
</div></blockquote>
<p>Note that Boost-MPI was used in BLock code.  It requires Boost being
compiled with the mpi components.  See <a class="reference external" href="http://www.boost.org/doc/libs/1_60_0/doc/html/mpi/getting_started.html">boost documents</a>
for details of the installation of Boost-MPI.</p>
<p>For certain compilers, you may have error message:</p>
<div class="highlight-python"><div class="highlight"><pre>error: ‘auto_ptr’ is deprecated (declared at /usr/include/c++/4.8.2/backward/auto_ptr.h:87) [-Werror=deprecated-declarations]
</pre></div>
</div>
<p>It is caused by the flag <tt class="docutils literal"><span class="pre">-Werror</span></tt>.  It is safe to remove this flag
from <tt class="docutils literal"><span class="pre">OPT</span></tt> variable.  Some compiler/linker may issue errors if
<tt class="docutils literal"><span class="pre">OPENMP</span> <span class="pre">=</span> <span class="pre">yes</span></tt> was specified in Makefile:</p>
<div class="highlight-python"><div class="highlight"><pre>/usr/bin/ld: dmrg.o: undefined reference to symbol &#39;shm_openn@@GLIBC_2.2.5&#39;
</pre></div>
</div>
<p>Appending <tt class="docutils literal"><span class="pre">-lpthread</span> <span class="pre">-lrt</span></tt> at the end of <tt class="docutils literal"><span class="pre">LIBS</span></tt> can solve this problem.</p>
<p>When the makefile is configured, run in the directory <tt class="docutils literal"><span class="pre">./Block</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>$ make
</pre></div>
</div>
<p>The successful compilation generates the executable <tt class="docutils literal"><span class="pre">block.spin_adapted</span></tt>, static and shared DMRG libraries <tt class="docutils literal"><span class="pre">libqcdmrg.a</span></tt> and <tt class="docutils literal"><span class="pre">libqcdmrg.so</span></tt>.</p>
<div class="section" id="interface-to-pyscf-package">
<span id="pyscf-itrf"></span><h3>Interface to PySCF package<a class="headerlink" href="#interface-to-pyscf-package" title="Permalink to this headline">¶</a></h3>
<p>The electronic structure Python module <a class="reference external" href="http://chemists.princeton.edu/chan/software/pyscf/">PySCF</a>
provided an interface to run <cite>BLOCK</cite> code.  If you would like to run
DMRG-SCF, DMRG-NEVPT2 etc with PySCF package, you need create a pyscf
config file <tt class="docutils literal"><span class="pre">/path/to/pyscf/future/dmrgscf/settings.py</span></tt> and add the
following settings in it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">BLOCKEXE</span> <span class="o">=</span> <span class="s">&quot;/path/to/Block/block.spin_adapted&quot;</span>
<span class="n">BLOCKEXE_COMPRESS_NEVPT</span> <span class="o">=</span> <span class="s">&quot;/path/to/serially/compiled/Block/block.spin_adapted&quot;</span>
<span class="n">BLOCKSCRATCHDIR</span> <span class="o">=</span> <span class="s">&quot;/path/to/scratch&quot;</span>
<span class="n">MPIPREFIX</span> <span class="o">=</span> <span class="s">&quot;mpirun&quot;</span>
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">BLOCKEXE</span></tt> is the parallel Block program. Most DMRG calculations (DMRG-CASCI,
DMRG-CASSCF etc) will call this parallel executable through <tt class="docutils literal"><span class="pre">mpirun</span></tt>
interface.  <tt class="docutils literal"><span class="pre">BLOCKEXE_COMPRESS_NEVPT</span></tt> points to the <strong>serially
compiled</strong> Block executable.  It is only needed by the compressed perturber
NEVPT2 method.  Although this Block executable file is not MPI-parallelized, the
DMRG-NEVPT2 program are efficiently parallelized in a different manner.
Note the parameter <tt class="docutils literal"><span class="pre">MPIPREFIX</span></tt> should be adjusted according to your
job scheduler, eg:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># For OpenPBS/Torque</span>
<span class="n">MPIPREFIX</span> <span class="o">=</span> <span class="s">&quot;mpirun&quot;</span>
<span class="c"># For SLURM</span>
<span class="n">MPIPREFIX</span> <span class="o">=</span> <span class="s">&quot;srun&quot;</span>
</pre></div>
</div>
<p>If calculation is carried out on interactive node, eg with 4 processors,
the setting looks like:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">MPIPREFIX</span> <span class="o">=</span> <span class="s">&quot;mpirun -n 4&quot;</span>
</pre></div>
</div>
<p>If <tt class="docutils literal"><span class="pre">BLOCK</span></tt> and <tt class="docutils literal"><span class="pre">PySCF</span></tt> are installed successfully, a simple DMRG-SCF
calculation can be input in Python interpereter:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyscf</span> <span class="kn">import</span> <span class="n">gto</span><span class="p">,</span> <span class="n">scf</span><span class="p">,</span> <span class="n">dmrgscf</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mf</span> <span class="o">=</span> <span class="n">gto</span><span class="o">.</span><span class="n">M</span><span class="p">(</span><span class="n">atom</span><span class="o">=</span><span class="s">&#39;C 0 0 0; C 0 0 1&#39;</span><span class="p">,</span> <span class="n">basis</span><span class="o">=</span><span class="s">&#39;ccpvdz&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">scf</span><span class="o">.</span><span class="n">RHF</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mc</span> <span class="o">=</span> <span class="n">dmrgscf</span><span class="o">.</span><span class="n">dmrgci</span><span class="o">.</span><span class="n">DMRGSCF</span><span class="p">(</span><span class="n">mf</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mc</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>DMRG-NEVPT2 calculation can be applied:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyscf</span> <span class="kn">import</span> <span class="n">mrpt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mrpt</span><span class="o">.</span><span class="n">NEVPT</span><span class="p">(</span><span class="n">mc</span><span class="p">)</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Optionally, if <a class="reference external" href="http://mpi4py.scipy.org">MPI4Py</a> was installed, the efficient
DMRG-NEVPT2 implementation can be used, eg:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pyscf</span> <span class="kn">import</span> <span class="n">mrpt</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mrpt</span><span class="o">.</span><span class="n">NEVPT</span><span class="p">(</span><span class="n">mc</span><span class="p">)</span><span class="o">.</span><span class="n">compress_approx</span><span class="p">()</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="how-to-run-block">
<h2>How to run <cite>BLOCK</cite><a class="headerlink" href="#how-to-run-block" title="Permalink to this headline">¶</a></h2>
<p>The standalone serial code can be executed running:</p>
<div class="highlight-python"><div class="highlight"><pre>$ block.spin_adapted input.dat &gt; output.dat
</pre></div>
</div>
<p><tt class="docutils literal"><span class="pre">input.dat</span></tt> is the input file and the output of the program is piped into the output file <tt class="docutils literal"><span class="pre">output.dat</span></tt>.</p>
<p>The MPI parallel mode can be called running:</p>
<div class="highlight-python"><div class="highlight"><pre>$ mpirun -np 4 block.spin_adapted input.dat &gt; output.dat
</pre></div>
</div>
</div>
<div class="section" id="testjobs">
<h2>Testjobs<a class="headerlink" href="#testjobs" title="Permalink to this headline">¶</a></h2>
<p><cite>BLOCK</cite> can be tested by executing the script in the directory <tt class="docutils literal"><span class="pre">./Block/dmrg_tests</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>$ cd dmrg_tests
$ ./runtest
</pre></div>
</div>
<p>The tests require Python to be installed on the system.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="examples.html" title="Typical Calculations with BLOCK"
             >next</a></li>
        <li class="right" >
          <a href="overview.html" title="Overview"
             >previous</a> |</li>
        <li><a href="index.html">BLOCK 1.5.0 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2015, Garnet Kin-Lic Chan.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>